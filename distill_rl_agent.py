#!/usr/bin/env python
"""
distill_rl_agent.py

This script distills an RL teacher into a student model via supervised learning.
It performs the following steps:
  1. Self‑Play Data Collection:
       Two copies of the RL teacher agent play against each other.
       Their observations are recorded, and for each observation the teacher’s 
       masked soft target distribution is computed at a high temperature.
  2. Synthetic Data Generation:
       Synthetic observations are generated with modifications:
         - Virtual hand: 1–15 cards sampled from a shuffled standard deck.
         - “Sorts played” (indices 100–104): generated by taking a random total (0–70)
           and assigning each sort a random 10–40% fraction of that total.
         - For player card–counts (indices 106–118): only two players are simulated.
           Our count is the virtual hand size; the opponent’s is a random number between 1 and 15.
         - A random “current card” is selected to determine compatibility.
       For each synthetic observation the teacher’s masked soft target is computed at a high temperature.
       We generate 20k synthetic datapoints for playCard decisions and 20k for changeSort.
  3. Distillation Training:
       The self‑play and synthetic datasets are combined and split (90% training, 10% evaluation).
       The student (a larger variant of DenseSkipNet with an extra layer and doubled hidden widths)
       is trained via KL–divergence loss so that its temperature–scaled (and masked) output mimics the teacher’s.
  4. Simulation Evaluation:
       The distilled student is then evaluated by simulating games (here against a Random agent)
       to check if the distilled behavior transfers well into gameplay.
  5. The distilled student model is saved.
       
Ensure your PYTHONPATH includes the repository so that modules (game, deck, card, etc.) can be imported.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import random

# Import modules from the repository
import game
import deck
import card
from ppo2 import RL_SupervisedAgent  # original teacher agent class (unused in distillation below)
from supervisedAgent import DenseSkipNet

# -----------------------------------------------------------------------------
# Global settings
# -----------------------------------------------------------------------------
device = None         # torch device, set in main()
teacher_model = None  # the loaded teacher network
HIGH_TEMP = 2.0       # high temperature used for distillation

# -----------------------------------------------------------------------------
# Helper Functions: Action Mask and Soft Target Computation
# -----------------------------------------------------------------------------
def get_action_mask(obs):
    """
    Given an observation (numpy array or tensor), returns a valid–action mask (tensor of shape [55])
    on the global device. For playCard mode (obs[105]==0): valid actions come from counts in indices
    50..99 (mapped to actions 0..49) plus action index 54.
    For changeSort (obs[105]==1): valid actions are those corresponding to indices 50..53.
    """
    if isinstance(obs, torch.Tensor):
        obs_list = obs.detach().cpu().numpy().tolist()
    else:
        obs_list = obs
    mask = np.zeros(55, dtype=np.float32)
    if obs_list[105] == 0:
        for x in range(50, 100):
            if obs_list[x] >= 1:
                mask[x - 50] = 1
        mask[54] = 1
    else:
        for x in range(50, 54):
            mask[x] = 1
        mask[54] = 0
    return torch.tensor(mask, device=device, dtype=torch.float)

def compute_teacher_soft_target(obs, temperature):
    """
    Given an observation (as a numpy array or tensor), compute the teacher's soft target distribution.
    The teacher's output (logits) is masked using get_action_mask(), divided by the provided temperature,
    and then softmaxed.
    Returns a numpy array of shape (55,).
    """
    if not isinstance(obs, torch.Tensor):
        obs_tensor = torch.tensor(obs, dtype=torch.float, device=device)
    else:
        obs_tensor = obs.to(device)
    mask = get_action_mask(obs_tensor)
    with torch.no_grad():
        logits = teacher_model(obs_tensor.unsqueeze(0))  # shape (1, 55)
        masked_logits = torch.where(mask.bool(), logits, torch.tensor(-1e15, device=device))
        soft_logits = masked_logits / temperature
        teacher_probs = torch.softmax(soft_logits, dim=1).squeeze(0)
    return teacher_probs.cpu().numpy()

# -----------------------------------------------------------------------------
# Data Generation: Self-Play and Synthetic
# -----------------------------------------------------------------------------
def collect_self_play_data(num_points_target=100000):
    """
    Simulate games between two copies of the teacher agent and collect datapoints.
    For each stored observation from the episode, we recompute the teacher's soft target
    (with high temperature) and store the pair (observation, soft target).
    Separates playCard datapoints (flag 0) from changeSort datapoints (flag 1).
    """
    playcard_data = []
    changesort_data = []
    collected = 0

    # Local teacher agent wrapper; we use the existing RL_SupervisedAgent.
    def TeacherAgent(game_instance):
        return RL_SupervisedAgent(game_instance, teacher_model, device)

    print("Starting self-play data collection ...")
    while collected < num_points_target:
        g = game.Game([TeacherAgent, TeacherAgent])
        g.reset()
        g.auto_simulate(max_turns=500)
        for agent in g.players:
            for obs in agent.episode_obs:
                obs_np = obs.detach().cpu().numpy() if isinstance(obs, torch.Tensor) else np.array(obs)
                soft_target = compute_teacher_soft_target(obs_np, HIGH_TEMP)
                if obs_np[105] == 0:
                    playcard_data.append((obs_np, soft_target))
                    collected += 1
                else:
                    changesort_data.append((obs_np, soft_target))
        if collected % 10000 < 100:
            print(f"Collected {collected} playCard datapoints so far...")
    print("Self-play data collection complete.")
    return playcard_data, changesort_data

def generate_synthetic_observation(playcard=True):
    """
    Generate a synthetic observation (121-dim) with these modifications:
      - Virtual hand: sample a random hand size (1–15 cards) from a standard shuffled deck.
      - Card counts (indices 0..49): for each card in the hand, increment its count.
      - Compatibility counts (indices 50..99): for each card in the hand that is compatible with a
        randomly selected "current card", increment its count.
      - Sorts played (indices 100..104): generate a random total (0–70) and assign each sort an integer
        equal to a random 10–40% fraction of that total.
      - Mode flag (index 105): 0 for playCard mode, 1 for changeSort.
      - Player card counts (indices 106–118): simulate only two players.
          * Our count (index 106) equals our hand size.
          * Opponent's count (index 107) is a random integer between 1 and 15.
          * Remaining indices (108–118) are set to 0.
      - Player index (index 119): set to 0.
      - Game direction (index 120): set to 1.
    """
    obs = np.zeros(121, dtype=np.float32)
    # Virtual hand
    num_cards = random.randint(1, 15)
    d = deck.standardDeck()
    d.shuffle()
    hand_cards = d.cards[:num_cards]
    # Current card: choose one from the deck (after the hand)
    current_card = d.cards[num_cards] if len(d.cards) > num_cards else random.choice(d.cards)
    for c in hand_cards:
        obs[c.number] += 1
        if c.compatible(current_card.sort, current_card.truenumber):
            obs[c.number + 50] += 1
    # Sorts played: random total between 0 and 70, then each sort gets a 10–40% fraction.
    total_sort = random.randint(0, 70)
    for i in range(5):
        frac = random.uniform(0.1, 0.4)
        obs[100 + i] = int(total_sort * frac)
    # Mode flag
    obs[105] = 0 if playcard else 1
    # Player card counts: simulate 2 players.
    obs[106] = num_cards           # our count equals our hand size
    obs[107] = random.randint(1, 15) # opponent count
    for idx in range(108, 119):
        obs[idx] = 0
    # Player index and game direction.
    obs[119] = 0
    obs[120] = 1
    return obs

def generate_synthetic_data(num_points, playcard=True):
    """
    Generate synthetic datapoints. For each synthetic observation, compute the teacher's soft target
    (with high temperature) and store (obs, teacher_soft_target).
    """
    synthetic_data = []
    for _ in range(num_points):
        obs = generate_synthetic_observation(playcard=playcard)
        soft_target = compute_teacher_soft_target(obs, HIGH_TEMP)
        synthetic_data.append((obs, soft_target))
    return synthetic_data

# -----------------------------------------------------------------------------
# Student Model: Larger DenseSkipNet Variant
# -----------------------------------------------------------------------------
class StudentNet(nn.Module):
    """
    Student network: a larger variant of DenseSkipNet with an extra layer and doubled hidden widths.
    Here we use 8 hidden layers of 256 units each.
    """
    def __init__(self, input_dim=121, output_dim=55):
        super(StudentNet, self).__init__()
        hidden_dims = [256] * 8
        self.model = DenseSkipNet(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim, dropout_prob=0.3)
        
    def forward(self, x):
        return self.model(x)

# -----------------------------------------------------------------------------
# Training Function (using KL–divergence with temperature scaling and masked outputs)
# -----------------------------------------------------------------------------
def train_student_model(dataset, epochs=10, batch_size=256, learning_rate=1e-3):
    """
    Train the student model on the combined dataset using KL–divergence loss.
    The dataset consists of tuples (obs, teacher_soft_target), where teacher_soft_target is a probability vector.
    A 90%/10% train/evaluation split is used to monitor overfitting.
    
    The loss is computed as:
      KL( teacher || student ) = KLDivLoss( log_softmax(student_logits_masked/HIGH_TEMP), teacher_soft_target ) * (HIGH_TEMP^2)
    where student_logits_masked is obtained by masking the student logits using get_action_mask().
    """
    # Shuffle and split dataset
    random.shuffle(dataset)
    N = len(dataset)
    split_idx = int(0.9 * N)
    train_data = dataset[:split_idx]
    eval_data = dataset[split_idx:]
    
    # Prepare training tensors
    train_obs = np.array([d[0] for d in train_data])
    train_targets = np.array([d[1] for d in train_data])
    train_obs_tensor = torch.tensor(train_obs, dtype=torch.float)
    train_targets_tensor = torch.tensor(train_targets, dtype=torch.float)
    train_ds = TensorDataset(train_obs_tensor, train_targets_tensor)
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    
    # Prepare evaluation tensors
    eval_obs = np.array([d[0] for d in eval_data])
    eval_targets = np.array([d[1] for d in eval_data])
    eval_obs_tensor = torch.tensor(eval_obs, dtype=torch.float)
    eval_targets_tensor = torch.tensor(eval_targets, dtype=torch.float)
    eval_ds = TensorDataset(eval_obs_tensor, eval_targets_tensor)
    eval_loader = DataLoader(eval_ds, batch_size=batch_size, shuffle=False)
    
    # Instantiate student model
    student = StudentNet().to(device)
    optimizer = optim.Adam(student.parameters(), lr=learning_rate)
    criterion = nn.KLDivLoss(reduction='batchmean')
    
    student.train()
    print("Starting student training...")
    for ep in range(epochs):
        total_loss = 0.0
        for batch_obs, batch_targets in train_loader:
            batch_obs = batch_obs.to(device)
            batch_targets = batch_targets.to(device)
            optimizer.zero_grad()
            student_logits = student(batch_obs)  # shape (B, 55)
            # Compute mask for each sample in the batch.
            masks = torch.stack([get_action_mask(obs) for obs in batch_obs])
            # Mask the student logits: set invalid actions to -1e15.
            student_logits_masked = torch.where(masks.bool(), student_logits, torch.tensor(-1e15, device=device))
            # Compute log-softmax of masked student outputs at high temperature.
            student_log_probs = torch.log_softmax(student_logits_masked / HIGH_TEMP, dim=1)
            loss = criterion(student_log_probs, batch_targets) * (HIGH_TEMP ** 2)
            loss.backward()
            optimizer.step()
            total_loss += loss.item() * batch_obs.size(0)
        avg_loss = total_loss / len(train_ds)
        
        # Evaluate on held-out set
        student.eval()
        eval_loss = 0.0
        with torch.no_grad():
            for batch_obs, batch_targets in eval_loader:
                batch_obs = batch_obs.to(device)
                batch_targets = batch_targets.to(device)
                student_logits = student(batch_obs)
                masks = torch.stack([get_action_mask(obs) for obs in batch_obs])
                student_logits_masked = torch.where(masks.bool(), student_logits, torch.tensor(-1e15, device=device))
                student_log_probs = torch.log_softmax(student_logits_masked / HIGH_TEMP, dim=1)
                loss = criterion(student_log_probs, batch_targets) * (HIGH_TEMP ** 2)
                eval_loss += loss.item() * batch_obs.size(0)
        avg_eval_loss = eval_loss / len(eval_ds)
        student.train()
        
        print(f"Epoch {ep+1}/{epochs} - Train Loss: {avg_loss:.4f} - Eval Loss: {avg_eval_loss:.4f}")
    print("Student training complete.")
    return student

# -----------------------------------------------------------------------------
# Simulation Evaluation: Student Agent
# -----------------------------------------------------------------------------
class StudentAgent:
    """
    A simulation agent that uses the distilled student model.
    Implements a similar interface to RL_SupervisedAgent.
    """
    def __init__(self, game_instance, model, device):
        self.game = game_instance
        self.model = model  # distilled student network
        self.device = device
        self.mydeck = deck.Deck([])
        self.type = "Student_RL"
        self.eval_mode = True

    def obs(self):
        # Build observation vector similar to RL_SupervisedAgent.
        obs = torch.zeros(121, dtype=torch.float)
        for c in self.mydeck.cards:
            obs[c.number] += 1
            if c.compatible(self.game.current_sort, self.game.current_true_number):
                obs[c.number + 50] += 1
        for i in range(5):
            obs[100 + i] = self.game.sorts_played[i]
        obs[105] = 0
        try:
            ai_index = self.game.players.index(self)
        except ValueError:
            ai_index = 0
        order = []
        cur_index = ai_index
        for _ in range(len(self.game.players)):
            order.append(cur_index)
            cur_index = self.game.calculate_next_player(cur_index, self.game.direction)
        idx = 106
        for player_index in order:
            player = self.game.players[player_index]
            obs[idx] = player.mydeck.cardCount()
            idx += 1
        try:
            obs[119] = self.game.players.index(self)
        except ValueError:
            obs[119] = -1
        obs[120] = self.game.direction
        return obs

    def get_action_mask(self, obs):
        return get_action_mask(obs)

    def act(self, obs, temperature=1e-3):
        if not isinstance(obs, torch.Tensor):
            obs = torch.tensor(obs, dtype=torch.float, device=self.device)
        obs = obs.to(self.device)
        obs_batched = obs.unsqueeze(0)
        logits = self.model(obs_batched).squeeze(0)
        mask = self.get_action_mask(obs)
        masked_logits = torch.where(mask.bool(), logits, torch.tensor(-1e15, device=self.device))
        scaled_logits = masked_logits / temperature
        action = torch.argmax(scaled_logits).item()
        return action

    def playCard(self, current_sort, current_true_number, temperature=1e-3):
        observation = self.obs()
        action = self.act(observation, temperature)
        if action == 54:
            return None
        else:
            return card.Card(action)

    def changeSort(self):
        observation = self.obs()
        observation[105] = 1
        action = self.act(observation, temperature=1e-3)
        return card.sorts[action - 50]

    def addCard(self, _card):
        self.mydeck.cards.append(_card)

    def remove(self, _card):
        for c in self.mydeck.cards:
            if c.number == _card.number:
                self.mydeck.cards.remove(c)
                break

def evaluate_student_model(student_model, num_games=100):
    """
    Evaluate the distilled student model by simulating games.
    The student agent (wrapped in StudentAgent) plays against a Random agent.
    Reports win rate and average game length.
    """
    from randomAgent import Agent as RandomAgent

    wins = 0
    total_turns = 0

    # Wrapper to create a StudentAgent using the distilled student model.
    def StudentAgentWrapper(game_instance):
        return StudentAgent(game_instance, student_model, device)

    print("Starting simulation evaluation of the distilled student model...")
    for i in range(num_games):
        # For a two-player game, student is player 0 and opponent is random.
        agents = [StudentAgentWrapper, RandomAgent]
        g = game.Game(agents)
        g.reset()
        turns, winner = g.auto_simulate()
        total_turns += turns
        if winner == 0:
            wins += 1
    win_rate = wins / num_games * 100
    avg_turns = total_turns / num_games
    print(f"Student Evaluation over {num_games} games:")
    print(f"  Win Rate: {win_rate:.2f}%")
    print(f"  Average Turns per Game: {avg_turns:.2f}")

# -----------------------------------------------------------------------------
# Main: Load Teacher, Generate Data, Train, Evaluate, and Save Student Model
# -----------------------------------------------------------------------------
def main():
    global teacher_model, device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # --- Load Teacher Model ---
    print("Loading teacher (RL) model ...")
    teacher_model = DenseSkipNet(input_dim=121, output_dim=55, dropout_prob=0.3)
    teacher_model.load_state_dict(torch.load("ppo_actor.pth", map_location=device))
    teacher_model.to(device)
    teacher_model.eval()
    print("Teacher model loaded.")
    
    # --- Self-play Data Collection ---
    playcard_real, changesort_real = collect_self_play_data(num_points_target=100000)
    print(f"Self-play data collected: {len(playcard_real)} playCard and {len(changesort_real)} changeSort datapoints.")
    
    # --- Synthetic Data Generation ---
    print("Generating synthetic data for playCard decisions ...")
    synthetic_playcard = generate_synthetic_data(20000, playcard=True)
    print("Generating synthetic data for changeSort decisions ...")
    synthetic_changesort = generate_synthetic_data(50000, playcard=False)
    
    # --- Combine Datasets ---
    combined_dataset = playcard_real + synthetic_playcard + changesort_real + synthetic_changesort
    print(f"Total combined dataset size: {len(combined_dataset)} datapoints.")
    random.shuffle(combined_dataset)
    
    # --- Train Distilled (Student) Model ---
    student_model = train_student_model(combined_dataset, epochs=14, batch_size=256, learning_rate=1e-3)
    
    # --- Simulation Evaluation ---
    evaluate_student_model(student_model, num_games=2000)
    
    # --- Save Student Model ---
    torch.save(student_model.state_dict(), "distilled_rl_agent.pth")
    print("Distilled student model saved as 'distilled_rl_agent.pth'.")

if __name__ == "__main__":
    main()
